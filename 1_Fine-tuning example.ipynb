{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhGAT10AWkdO"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/upunaprosk/la-tda.git\n",
    "%cd la-tda\n",
    "!unzip data/data.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoIn_zHInuCP"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VXEbJc2eybX"
   },
   "outputs": [],
   "source": [
    "from grab_attentions import *\n",
    "from features_calculation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHgVLinqU9WL"
   },
   "source": [
    "# Fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSY-IKWIVP9g",
    "outputId": "052cc0ab-2692-4229-c9ac-342f252162ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  9 19:59:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   28C    P0    43W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyiOQFaPN7Ew"
   },
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr = 3e-5\n",
    "decay = 1e-2\n",
    "batch=32\n",
    "model_save_dir = \"./\"\n",
    "run_name = f\"bert-base-cased-en-cola_{batch}_{lr}_lr_{decay}_decay_balanced\"\n",
    "output_dir = model_save_dir+run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL8_AwTgWi9_"
   },
   "source": [
    "## Training argumens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjJP0MofWxgR"
   },
   "source": [
    "Training argumens include  \n",
    "\n",
    "\n",
    "[Trainer](https://github.com/huggingface/transformers/blob/d92e22d1f28324f513f3080e5c47c071a3916721/src/transformers/training_args.py#L121) class parameters;  \n",
    "Model type arguments;  \n",
    "\n",
    "```\n",
    "  --model_name_or_path MODEL_NAME_OR_PATH\n",
    "                        Path to pretrained model or model identifier from\n",
    "                        huggingface.co/models (default: None)\n",
    "  --config_name CONFIG_NAME\n",
    "                        Pretrained config name or path if not the same as\n",
    "                        model_name (default: None)\n",
    "  --tokenizer_name TOKENIZER_NAME\n",
    "                        Pretrained tokenizer name or path if not the same as\n",
    "                        model_name (default: None)\n",
    "```\n",
    "Data training arguments; \n",
    "\n",
    "\n",
    "```\n",
    "  --task_name TASK_NAME\n",
    "                        The name of the task to train on: cola, mnli, mrpc,\n",
    "                        qnli, qqp, rte, sst2, stsb, wnli (default: None)\n",
    "  --dataset_name DATASET_NAME\n",
    "                        The name of the dataset to use (via the datasets\n",
    "                        library). (default: None)\n",
    "  --max_seq_length MAX_SEQ_LENGTH\n",
    "                        The maximum total input sequence length after\n",
    "                        tokenization. Sequences longer than this will be\n",
    "                        truncated, sequences shorter will be padded. (default:\n",
    "                        128)\n",
    "  --train_file TRAIN_FILE\n",
    "                        A csv or a json file containing the training data.\n",
    "                        (default: None)\n",
    "  --validation_file VALIDATION_FILE\n",
    "                        A csv or a json file containing the validation data.\n",
    "                        (default: None)\n",
    "  --test_file TEST_FILE\n",
    "                        A csv or a json file containing the test data.\n",
    "                        (default: None)\n",
    "  --output_dir OUTPUT_DIR\n",
    "                        The output directory where the model predictions and\n",
    "                        checkpoints will be written. (default: None)\n",
    "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
    "                        Overwrite the content of the output directory. Use\n",
    "                        this to continue training if output_dir points to a\n",
    "                        checkpoint directory. (default: False)\n",
    "  --do_train [DO_TRAIN]\n",
    "                        Whether to run training. (default: False)\n",
    "  --do_eval [DO_EVAL]   Whether to run eval on the dev set. (default: False)\n",
    "  --do_predict [DO_PREDICT]\n",
    "                        Whether to run predictions on the test set. (default:\n",
    "                        False)\n",
    "  --evaluation_strategy {no,steps,epoch}\n",
    "                        The evaluation strategy to use. (default: no)\n",
    "```\n",
    "\n",
    "\n",
    "Balance loss function;\n",
    "```\n",
    "  --balance_loss        Whether to use class-balanced loss. (default: False)\n",
    "```\n",
    "Layers weights freezing;  \n",
    "\n",
    "\n",
    "```\n",
    "  --freeze              Whether to use pre-trained model without fine-tuning.\n",
    "                        (default: False)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueec-1WBdfKY",
    "outputId": "e57c0ec4-a13e-4efe-a4ae-e73af6164ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.py:48: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  ACCURACY = load_metric(\"accuracy\", keep_in_memory=True)\n",
      "Downloading builder script: 4.21kB [00:00, 4.98MB/s]       \n",
      "Downloading builder script: 4.47kB [00:00, 5.48MB/s]       \n",
      "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/runs/Oct09_17-40-25_a667bc2ba445,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "INFO:__main__:load a local file for train: data/en-cola/train.csv\n",
      "INFO:__main__:load a local file for validation: data/en-cola/dev.csv\n",
      "INFO:__main__:load a local file for test: data/en-cola/test.csv\n",
      "WARNING:datasets.builder:Using custom data configuration default-e8047ae3a6efaf5c\n",
      "INFO:datasets.builder:Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-e8047ae3a6efaf5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e8047ae3a6efaf5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n",
      "Downloading data files: 100% 3/3 [00:00<00:00, 12852.82it/s]\n",
      "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
      "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
      "Extracting data files: 100% 3/3 [00:00<00:00, 1721.09it/s]\n",
      "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
      "INFO:datasets.builder:Generating train split\n",
      "INFO:datasets.builder:Generating validation split\n",
      "INFO:datasets.builder:Generating test split\n",
      "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e8047ae3a6efaf5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
      "100% 3/3 [00:00<00:00, 853.72it/s]\n",
      "Downloading: 100% 570/570 [00:00<00:00, 680kB/s]\n",
      "[INFO|configuration_utils.py:651] 2022-10-09 17:40:26,023 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/config.json\n",
      "[INFO|configuration_utils.py:703] 2022-10-09 17:40:26,027 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Downloading: 100% 29.0/29.0 [00:00<00:00, 29.9kB/s]\n",
      "[INFO|configuration_utils.py:651] 2022-10-09 17:40:26,282 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/config.json\n",
      "[INFO|configuration_utils.py:703] 2022-10-09 17:40:26,282 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Downloading: 100% 213k/213k [00:00<00:00, 2.14MB/s]\n",
      "Downloading: 100% 436k/436k [00:00<00:00, 3.06MB/s]\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-09 17:40:27,376 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-09 17:40:27,376 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-09 17:40:27,376 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-09 17:40:27,376 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-09 17:40:27,376 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:651] 2022-10-09 17:40:27,376 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/config.json\n",
      "[INFO|configuration_utils.py:703] 2022-10-09 17:40:27,377 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Downloading: 100% 436M/436M [00:06<00:00, 72.5MB/s]\n",
      "[INFO|modeling_utils.py:2088] 2022-10-09 17:40:33,720 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2529] 2022-10-09 17:40:35,163 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2541] 2022-10-09 17:40:35,163 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e8047ae3a6efaf5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-0b3828052cbb0a5d.arrow\n",
      "Running tokenizer on dataset: 100% 9/9 [00:00<00:00, 27.48ba/s]\n",
      "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e8047ae3a6efaf5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-20f1f1265d2e58ce.arrow\n",
      "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 44.71ba/s]\n",
      "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e8047ae3a6efaf5c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-665c595135007dc1.arrow\n",
      "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 49.79ba/s]\n",
      "INFO:__main__:Sample 1824 of the training set: {'code': 'r-67', 'label': 0, 'judgement': '?*', 'sentence': 'I acknowledged that my father, he was tight as an owl.', 'input_ids': [101, 146, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "INFO:__main__:Sample 409 of the training set: {'code': 'bc01', 'label': 1, 'judgement': None, 'sentence': 'For him to do that would be a mistake.', 'input_ids': [101, 1370, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "INFO:__main__:Sample 4506 of the training set: {'code': 'ks08', 'label': 1, 'judgement': None, 'sentence': 'Mary sang a song, but Lee never did.', 'input_ids': [101, 2090, 6407, 170, 1461, 117, 1133, 2499, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "INFO:__main__:Assigned weights to classes\n",
      "INFO:__main__:{1.6912579113924051: 0, 0.7098621949194753: 1}\n",
      "[INFO|trainer.py:726] 2022-10-09 17:40:42,274 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-10-09 17:40:42,292 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-10-09 17:40:42,292 >>   Num examples = 8551\n",
      "[INFO|trainer.py:1609] 2022-10-09 17:40:42,292 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1610] 2022-10-09 17:40:42,292 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1611] 2022-10-09 17:40:42,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1612] 2022-10-09 17:40:42,292 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-10-09 17:40:42,292 >>   Total optimization steps = 268\n",
      "100% 267/268 [00:31<00:00,  9.20it/s][INFO|trainer.py:1852] 2022-10-09 17:41:14,194 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 31.9022, 'train_samples_per_second': 268.038, 'train_steps_per_second': 8.401, 'train_loss': 0.5624045186967992, 'epoch': 1.0}\n",
      "100% 268/268 [00:31<00:00,  8.40it/s]\n",
      "[INFO|trainer.py:2656] 2022-10-09 17:41:14,196 >> Saving model checkpoint to ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced\n",
      "[INFO|configuration_utils.py:445] 2022-10-09 17:41:14,196 >> Configuration saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/config.json\n",
      "[INFO|modeling_utils.py:1583] 2022-10-09 17:41:15,024 >> Model weights saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-09 17:41:15,025 >> tokenizer config file saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-09 17:41:15,025 >> Special tokens file saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.5624\n",
      "  train_runtime            = 0:00:31.90\n",
      "  train_samples            =       8551\n",
      "  train_samples_per_second =    268.038\n",
      "  train_steps_per_second   =      8.401\n",
      "INFO:__main__:*** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-10-09 17:41:15,059 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-09 17:41:15,061 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-09 17:41:15,061 >>   Num examples = 527\n",
      "[INFO|trainer.py:2912] 2022-10-09 17:41:15,061 >>   Batch size = 8\n",
      "100% 66/66 [00:00<00:00, 74.94it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =      0.833\n",
      "  eval_loss               =     0.4745\n",
      "  eval_mcc                =     0.5902\n",
      "  eval_runtime            = 0:00:00.90\n",
      "  eval_samples            =        527\n",
      "  eval_samples_per_second =    582.918\n",
      "  eval_steps_per_second   =     73.003\n",
      "INFO:__main__:*** Predict ***\n",
      "[INFO|trainer.py:726] 2022-10-09 17:41:15,969 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-09 17:41:15,970 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-09 17:41:15,970 >>   Num examples = 8551\n",
      "[INFO|trainer.py:2912] 2022-10-09 17:41:15,970 >>   Batch size = 8\n",
      "100% 1069/1069 [00:13<00:00, 78.09it/s]\n",
      "INFO:__main__:***** Predict results train *****\n",
      "[INFO|trainer.py:726] 2022-10-09 17:41:29,679 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-09 17:41:29,680 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-09 17:41:29,680 >>   Num examples = 527\n",
      "[INFO|trainer.py:2912] 2022-10-09 17:41:29,680 >>   Batch size = 8\n",
      "100% 66/66 [00:00<00:00, 79.52it/s]\n",
      "INFO:__main__:***** Predict results dev *****\n",
      "[INFO|trainer.py:726] 2022-10-09 17:41:30,526 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-09 17:41:30,527 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-09 17:41:30,527 >>   Num examples = 516\n",
      "[INFO|trainer.py:2912] 2022-10-09 17:41:30,527 >>   Batch size = 8\n",
      "100% 65/65 [00:00<00:00, 79.57it/s]\n",
      "INFO:__main__:***** Predict results test *****\n",
      "[INFO|modelcard.py:443] 2022-10-09 17:41:31,520 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8330170777988615}]}\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "        --model_name_or_path bert-base-cased \\\n",
    "        --train_file data/en-cola/train.csv \\\n",
    "        --validation_file data/en-cola/dev.csv \\\n",
    "        --test_file data/en-cola/test.csv \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --do_predict\\\n",
    "        --num_train_epochs $epoch\\\n",
    "        --learning_rate $lr\\\n",
    "        --weight_decay $decay\\\n",
    "        --max_seq_length 64\\\n",
    "        --per_device_train_batch_size $batch\\\n",
    "        --output_dir $output_dir\\\n",
    "        --balance_loss\\\n",
    "        # --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeCSemL-ZLhR"
   },
   "source": [
    "# Attention weights extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJ1Tn9hFS-JT"
   },
   "outputs": [],
   "source": [
    "# texts that extract attentions for\n",
    "d_dir = f\"./data/en-cola/dev.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBNhb8lETCZ6"
   },
   "outputs": [],
   "source": [
    "!PYTHONPATH=%PYTHONPATH% python grab_attentions.py --model_dir $output_dir --data_file $d_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rowE-Mk0TMUr"
   },
   "outputs": [],
   "source": [
    "# Ex. of direct function call\n",
    "# grab_attention_weights_inference(output_dir, d_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to ./feature_src/ notebooks for feature calculation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
