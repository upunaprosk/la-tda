{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for topological and template features calculation.\n",
    "Based on [the code](https://github.com/danchern97/tda4atd/blob/main/features_calculation_by_thresholds.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import torch\n",
    "import gzip\n",
    "import networkx as nx\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_matrix(matrix, ntokens):\n",
    "    \"\"\"Return normalized submatrix of first n_tokens\"\"\"\n",
    "    matrix = matrix[:ntokens, :ntokens]\n",
    "    matrix /= matrix.sum(axis=1, keepdims=True)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_mat_list(adj_matrix, thresholds_array, ntokens):\n",
    "    \"\"\"\n",
    "    Converts adjacency matrix with real weights into list of binary matrices.\n",
    "    For each threshold, those weights of adjacency matrix, which are less than\n",
    "    threshold, get \"filtered out\" (set to 0), remained weights are set to ones.\n",
    "\n",
    "    Args:\n",
    "        adj_matrix (np.array[float, float])\n",
    "        thresholds_array (iterable[float])\n",
    "        ntokens (int)\n",
    "\n",
    "    Returns:\n",
    "        filtered_matricies (list[np.array[int, int]])\n",
    "    \"\"\"\n",
    "    filtered_matrices = []\n",
    "    for thr in thresholds_array:\n",
    "        filtered_matrix = adj_matrix.copy()\n",
    "        filtered_matrix = cutoff_matrix(filtered_matrix, ntokens)\n",
    "        filtered_matrix[filtered_matrix < thr] = 0\n",
    "        filtered_matrix[filtered_matrix >= thr] = 1\n",
    "        filtered_matrices.append(filtered_matrix.astype(np.int8))\n",
    "    return filtered_matrices\n",
    "\n",
    "\n",
    "def adj_m_to_nx_list(adj_matrix, thresholds_array, ntokens, no_mat_output=False):\n",
    "    \"\"\"\n",
    "    Converts adjacency matrix into list of unweighted digraphs, using filtering\n",
    "    process from previous function.\n",
    "\n",
    "    Args:\n",
    "        adj_matrix (np.array[float, float])\n",
    "        thresholds_array (iterable[float])\n",
    "        ntokens (int)\n",
    "\n",
    "    Returns:\n",
    "        nx_graphs_list (list[nx.MultiDiGraph])\n",
    "        filt_mat_list(list[np.array[int, int]])\n",
    "\n",
    "    \"\"\"\n",
    "    #     adj_matrix = adj_matrix[:length,:length]\n",
    "    filt_mat_list = get_filtered_mat_list(adj_matrix, thresholds_array, ntokens)\n",
    "    nx_graphs_list = []\n",
    "    for mat in filt_mat_list:\n",
    "        nx_graphs_list.append(nx.from_numpy_matrix(np.array(mat),\n",
    "                                                   create_using=nx.MultiDiGraph()))\n",
    "    if no_mat_output:\n",
    "        return nx_graphs_list, []\n",
    "    else:\n",
    "        return nx_graphs_list, filt_mat_list\n",
    "\n",
    "\n",
    "def adj_ms_to_nx_lists(adj_matricies, \\\n",
    "                       thresholds_array, \\\n",
    "                       ntokens_array, \\\n",
    "                       verbose=False, \\\n",
    "                       no_mat_output=False):\n",
    "    \"\"\"\n",
    "    Executes adj_m_to_nx_list for each matrix in adj_matricies array, arranges\n",
    "    the results. If verbose==True, shows progress bar.\n",
    "\n",
    "    Args:\n",
    "        adj_matricies (np.array[float, float])\n",
    "        thresholds_array (iterable[float])\n",
    "        verbose (bool)\n",
    "\n",
    "    Returns:\n",
    "        nx_graphs_list (list[nx.MultiDiGraph])\n",
    "        filt_mat_lists (list[list[np.array[int,int]]])\n",
    "    \"\"\"\n",
    "    graph_lists = []\n",
    "    filt_mat_lists = []\n",
    "\n",
    "    iterable = range(len(adj_matricies))\n",
    "    if verbose:\n",
    "        iterable = tqdm(range(len(adj_matricies)),\n",
    "                        desc=\"Calc graphs list\")\n",
    "    for i in iterable:\n",
    "        g_list, filt_mat_list = adj_m_to_nx_list(adj_matricies[i], \\\n",
    "                                                 thresholds_array, \\\n",
    "                                                 ntokens_array[i], \\\n",
    "                                                 no_mat_output=no_mat_output)\n",
    "        graph_lists.append(g_list)\n",
    "        filt_mat_lists.append(filt_mat_lists)\n",
    "\n",
    "    return graph_lists, filt_mat_lists\n",
    "\n",
    "\n",
    "def count_stat(g_listt_j, function=nx.weakly_connected_components, cap=500):\n",
    "    stat_amount = 0\n",
    "    for _ in function(g_listt_j):\n",
    "        stat_amount += 1\n",
    "        if stat_amount >= cap:\n",
    "            break\n",
    "    return stat_amount\n",
    "\n",
    "\n",
    "def count_weak_components(g_listt_j, cap=500):\n",
    "    return count_stat(g_listt_j, function=nx.weakly_connected_components, cap=cap)\n",
    "\n",
    "\n",
    "def count_strong_components(g_listt_j, cap=500):\n",
    "    return count_stat(g_listt_j, function=nx.strongly_connected_components, cap=cap)\n",
    "\n",
    "\n",
    "def count_simple_cycles(g_listt_j, cap=500):\n",
    "    return count_stat(g_listt_j, function=nx.simple_cycles, cap=cap)\n",
    "\n",
    "\n",
    "def count_b1(g_listt_j, cap=500):\n",
    "    return count_stat(g_listt_j, function=nx.cycle_basis, cap=cap)\n",
    "\n",
    "\n",
    "def dim_connected_components(graph_lists, strong=False, verbose=False, cap=500):\n",
    "    \"\"\"\n",
    "    Calculates number of connected components for each graph in list\n",
    "    of lists of digraphs. If strong==True, calculates strongly connected\n",
    "    components, otherwise calculates weakly connected components.\n",
    "    If verbose==True, shows progress bar.\n",
    "\n",
    "    Args:\n",
    "        graph_lists (list[list[nx.MultiDiGraph]])\n",
    "        strong (bool)\n",
    "        verbose (bool)\n",
    "\n",
    "    Returns:\n",
    "        w_lists (list[list[int])\n",
    "    \"\"\"\n",
    "    w_lists = []  # len == len(w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    if verbose:\n",
    "        iterable = tqdm(range(len(graph_lists)),\n",
    "                        desc=\"Calc weak comp\")\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        w_cmp = []\n",
    "        for j in range(len(g_list)):\n",
    "            if strong:\n",
    "                w_cmp.append(count_strong_components(g_list[j], cap=cap))\n",
    "            else:\n",
    "                w_cmp.append(count_weak_components(g_list[j], cap=cap))\n",
    "        w_lists.append(w_cmp)\n",
    "    return w_lists\n",
    "\n",
    "\n",
    "def dim_simple_cycles(graph_lists, verbose, cap=500):\n",
    "    \"\"\"\n",
    "    Calculates number of simple cycles for each graph in list\n",
    "    of lists of digraphs. If verbose==True, shows progress bar.\n",
    "\n",
    "    Args:\n",
    "        graph_lists (list[list[nx.MultiDiGraph]])\n",
    "        verbose (bool)\n",
    "\n",
    "    Returns:\n",
    "        c_lists (list[list[int])\n",
    "    \"\"\"\n",
    "    c_lists = []  # len == len(pos_w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    if verbose:\n",
    "        iterable = tqdm(range(len(graph_lists)),\n",
    "                        desc=\"Calc cycles\")\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        c = []\n",
    "        for j in range(len(g_list)):\n",
    "            c.append(count_simple_cycles(g_list[j], cap=cap))\n",
    "        c_lists.append(c)\n",
    "    if verbose:\n",
    "        logger = logging.getLogger()\n",
    "        flat = [x for l in c_lists for x in l]\n",
    "        logger.debug('dim_simple_cycles: min=%f mean=%f max=%f,  ratio of cap = %d / %d' %\n",
    "                     (\n",
    "                         np.min(c_lists), np.mean(c_lists), np.max(c_lists), len([x for x in flat if x == cap]),\n",
    "                         len(flat)))\n",
    "\n",
    "    return c_lists\n",
    "\n",
    "\n",
    "def dim_b1(graph_lists, verbose, cap=500):\n",
    "    b1_lists = []  # len == len(pos_w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    if verbose:\n",
    "        iterable = tqdm(range(len(graph_lists)),\n",
    "                        desc=\"Calc b1 (undirected graphs)\")\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        b1 = []\n",
    "        for j in range(len(g_list)):\n",
    "            b1.append(count_b1(nx.Graph(g_list[j].to_undirected()), cap=cap))\n",
    "        b1_lists.append(b1)\n",
    "    return b1_lists\n",
    "\n",
    "\n",
    "def b0_b1(graph_lists, verbose):\n",
    "    b0_lists = []\n",
    "    b1_lists = []  # len == len(pos_w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    if verbose:\n",
    "        iterable = tqdm(range(len(graph_lists)),\n",
    "                        desc=\"Calc b0, b1\")\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        b0 = []\n",
    "        b1 = []\n",
    "        for j in range(len(g_list)):\n",
    "            g = nx.Graph(g_list[j].to_undirected())\n",
    "            w = nx.number_connected_components(g)\n",
    "            e = g.number_of_edges()\n",
    "            v = g.number_of_nodes()\n",
    "            b0.append(w)\n",
    "            b1.append(e - v + w)\n",
    "        b0_lists.append(b0)\n",
    "        b1_lists.append(b1)\n",
    "    return b0_lists, b1_lists\n",
    "\n",
    "\n",
    "def edges_f(graph_lists, verbose):\n",
    "    \"\"\"\n",
    "    Calculates number of edges for each graph in list\n",
    "    of lists of digraphs. If verbose==True, shows progress bar.\n",
    "\n",
    "    Args:\n",
    "        graph_lists (list[list[nx.MultiDiGraph]])\n",
    "        verbose (bool)\n",
    "\n",
    "    Returns:\n",
    "        e_lists (list[list[int])\n",
    "    \"\"\"\n",
    "    e_lists = []  # len == len(pos_w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    if verbose > 2:\n",
    "        iterable = tqdm(range(len(graph_lists)),\n",
    "                        desc=\"Calc edges number\")\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        e = []\n",
    "        for j in range(len(g_list)):\n",
    "            e.append(g_list[j].number_of_edges())\n",
    "        e_lists.append(e)\n",
    "    return e_lists\n",
    "\n",
    "\n",
    "def v_degree_f(graph_lists, verbose):\n",
    "    \"\"\"\n",
    "    Calculates number of edges for each graph in list\n",
    "    of lists of digraphs. If verbose==True, shows progress bar.\n",
    "\n",
    "    Args:\n",
    "        graph_lists (list[list[nx.MultiDiGraph]])\n",
    "        verbose (bool)\n",
    "\n",
    "    Returns:\n",
    "        v_lists (list[list[int])\n",
    "    \"\"\"\n",
    "    v_lists = []  # len == len(pos_w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    if verbose > 2:\n",
    "        iterable = tqdm(range(len(graph_lists)),\n",
    "                        desc=\"Calc average vertex degree\")\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        v = []\n",
    "        for j in range(len(g_list)):\n",
    "            degrees = g_list[j].degree()\n",
    "            degree_values = [v for k, v in degrees]\n",
    "            sum_of_edges = sum(degree_values) / float(len(degree_values))\n",
    "            v.append(sum_of_edges)\n",
    "        v_lists.append(v)\n",
    "    return v_lists\n",
    "\n",
    "\n",
    "def chordality_f(graph_lists, verbose):\n",
    "    \"\"\"\n",
    "    Checks whether the graph is chordal or not for each graph in list of lists of graphs.\n",
    "    If verbose==True, shows progress bar.\n",
    "\n",
    "    Args:\n",
    "        graph_lists (list[list[nx.MultiDiGraph]])\n",
    "        verbose (bool)\n",
    "\n",
    "    Returns:\n",
    "        ch_lists (list[list[int])\n",
    "    \"\"\"\n",
    "    ch_lists = []  # len == len(pos_w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        ch = []\n",
    "        for j in range(len(g_list)):\n",
    "            g = nx.Graph(g_list[j].to_undirected())\n",
    "            # print(g)\n",
    "            # print(g.edges())\n",
    "            g.remove_edges_from(nx.selfloop_edges(g))\n",
    "            ch_i = nx.is_chordal(g)\n",
    "            ch.append(int(ch_i))\n",
    "        ch_lists.append(ch)\n",
    "    return ch_lists\n",
    "def max_matching_f(graph_lists, verbose):\n",
    "    \"\"\"\n",
    "    Calculates max matching size for each graph in list\n",
    "    of lists of graphs. If verbose==True, shows progress bar.\n",
    "\n",
    "    Args:\n",
    "        graph_lists (list[list[nx.MultiDiGraph]])\n",
    "        verbose (bool)\n",
    "\n",
    "    Returns:\n",
    "        max_m_lists (list[list[int])\n",
    "    \"\"\"\n",
    "    max_m_lists = []  # len == len(pos_w_graph_lists)\n",
    "    iterable = range(len(graph_lists))\n",
    "    if verbose > 2:\n",
    "        iterable = tqdm(range(len(graph_lists)),\n",
    "                        desc=\"Calc max matching of the graph\")\n",
    "    for i in iterable:\n",
    "        g_list = graph_lists[i]\n",
    "        max_m = []\n",
    "        for j in range(len(g_list)):\n",
    "            g = nx.Graph(g_list[j].to_undirected())\n",
    "            m_i = nx.maximal_matching(g)\n",
    "            max_m.append(len(m_i))\n",
    "        max_m_lists.append(max_m)\n",
    "    return max_m_lists\n",
    "\n",
    "def count_top_stats(adj_matricies,\n",
    "                    thresholds_array,\n",
    "                    ntokens_array,\n",
    "                    stats_to_count={\"s\", \"e\", \"c\", \"v\", \"b0b1\"},\n",
    "                    stats_cap=500,\n",
    "                    verbose=False):\n",
    "    \"\"\"\n",
    "    The main function for calculating topological invariants. Unites the\n",
    "    functional of all functions above.\n",
    "    Args:\n",
    "        adj_matricies (np.array[float, float, float, float, float])\n",
    "        thresholds_array (list[float])\n",
    "        stats_to_count (str)\n",
    "        stats_cap (int)\n",
    "        verbose (bool)\n",
    "    Returns:\n",
    "        stats_tuple_lists_array (np.array[float, float, float, float, float])\n",
    "    \"\"\"\n",
    "    stats_tuple_lists_array = []\n",
    "\n",
    "    for layer_of_interest in tqdm(range(adj_matricies.shape[1])):\n",
    "        stats_tuple_lists_array.append([])\n",
    "        for head_of_interest in range(adj_matricies.shape[2]):\n",
    "            adj_ms = adj_matricies[:, layer_of_interest, head_of_interest, :, :]\n",
    "            g_lists, _ = adj_ms_to_nx_lists(adj_ms,\n",
    "                                            thresholds_array=thresholds_array,\n",
    "                                            ntokens_array=ntokens_array,\n",
    "                                            verbose=verbose)\n",
    "            feat_lists = []\n",
    "            if \"s\" in stats_to_count:\n",
    "                feat_lists.append(dim_connected_components(g_lists,\n",
    "                                                           strong=True,\n",
    "                                                           verbose=verbose,\n",
    "                                                           cap=stats_cap))\n",
    "            if \"w\" in stats_to_count:\n",
    "                feat_lists.append(dim_connected_components(g_lists,\n",
    "                                                           strong=False,\n",
    "                                                           verbose=verbose,\n",
    "                                                           cap=stats_cap))\n",
    "            if \"e\" in stats_to_count:\n",
    "                feat_lists.append(edges_f(g_lists, verbose=verbose))\n",
    "            if \"v\" in stats_to_count:\n",
    "                feat_lists.append(v_degree_f(g_lists, verbose=verbose))\n",
    "            if \"c\" in stats_to_count:\n",
    "                feat_lists.append(dim_simple_cycles(g_lists,\n",
    "                                                    verbose=verbose,\n",
    "                                                    cap=50))\n",
    "\n",
    "            if \"b0b1\" in stats_to_count:\n",
    "                b0_lists, b1_lists = b0_b1(g_lists, verbose=verbose)\n",
    "                feat_lists.append(b0_lists)\n",
    "                feat_lists.append(b1_lists)\n",
    "            if \"m\" in stats_to_count:\n",
    "                feat_lists.append(max_matching_f(g_lists,\n",
    "                                                    verbose=verbose))\n",
    "            if \"k\" in stats_to_count:\n",
    "                feat_lists.append(chordality_f(g_lists,\n",
    "                                                    verbose=verbose))\n",
    "            stats_tuple_lists_array[-1].append(tuple(feat_lists))\n",
    "\n",
    "    stats_tuple_lists_array = np.asarray(stats_tuple_lists_array,\n",
    "                                         dtype=np.float16)\n",
    "    return stats_tuple_lists_array\n",
    "\n",
    "\n",
    "def function_for_v(list_of_v_degrees_of_graph):\n",
    "    return sum(map(lambda x: np.sqrt(x * x), list_of_v_degrees_of_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jMnrS4q4SkqF"
   },
   "outputs": [],
   "source": [
    "subset = \"test_sub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "73YlQ-nmmzwt"
   },
   "outputs": [],
   "source": [
    "model_path = \"./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z-3gZpVz2lxX"
   },
   "outputs": [],
   "source": [
    "input_dir = output_dir = './'\n",
    "layers_of_interest = list(range(24))\n",
    "data = pd.read_csv(\"./data/en-cola/\" + subset + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "s5f1AsPn2rmp"
   },
   "outputs": [],
   "source": [
    "def get_token_length(batch_texts):\n",
    "    inputs = tokenizer.batch_encode_plus(batch_texts,\n",
    "       return_tensors='pt',\n",
    "       add_special_tokens=True,\n",
    "       max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "       pad_to_max_length=True,         # Pad sentence to max length\n",
    "       truncation=True\n",
    "    )\n",
    "    inputs = inputs['input_ids'].numpy()\n",
    "    n_tokens = []\n",
    "    indexes = np.argwhere(inputs == tokenizer.pad_token_id)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        ids = indexes[(indexes == i)[:, 0]]\n",
    "        if not len(ids):\n",
    "            n_tokens.append(MAX_LEN)\n",
    "        else:\n",
    "            n_tokens.append(ids[0, 1])\n",
    "    return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-PmbEU-jXsZc"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "max_tokens_amount  = 64\n",
    "MAX_LEN = max_tokens_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KFtjQMDmXrd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "data['tokenizer_length'] = get_token_length(list(data['sentence'].values))\n",
    "ntokens_array = data['tokenizer_length'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hMjuDds1zndK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/features/test_subs_e_v_c_b0b1_m_k_array_6.npy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_name = \"s_e_v_c_b0b1_m_k\"\n",
    "thresholds_array = [0.025, 0.05, 0.1, 0.25, 0.5, 0.75]\n",
    "thrs = len(thresholds_array)\n",
    "layers_of_interest = [i for i in range(12)]\n",
    "stats_file = model_path + 'features/' + subset + \"\" + stats_name + \"_array_\" + str(thrs) + '.npy'\n",
    "stats_file             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_files(path, subset):\n",
    "    files_path = Path(path)\n",
    "    files = list(filter(lambda y: (y.is_file() and subset in str(y)), files_path.iterdir()))\n",
    "    files = [str(_) for _ in files]\n",
    "    files = sorted(files, key=lambda x: int(x.split('_')[-1].split('of')[0][4:].strip()))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/attentions/test_sub_part1of1.npy.gz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir=model_path\n",
    "attn_dir = model_path + \"/attentions/\"\n",
    "adj_filenames = order_files(path=attn_dir, subset=subset)\n",
    "adj_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Y122PsBY0NVn"
   },
   "outputs": [],
   "source": [
    "def split_matricies_and_lengths(adj_matricies, ntokens_array, num_of_workers):\n",
    "    splitted_adj_matricies = np.array_split(adj_matricies, num_of_workers)\n",
    "    splitted_ntokens = np.array_split(ntokens_array, num_of_workers)\n",
    "    assert all([len(m)==len(n) for m, n in zip(splitted_adj_matricies, splitted_ntokens)]), \"Split is not valid!\"\n",
    "    return zip(splitted_adj_matricies, splitted_ntokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pMVNj_LX0RCX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_workers = os.cpu_count()\n",
    "pool = Pool(num_of_workers)\n",
    "num_of_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xqGXgFxC0Scf"
   },
   "outputs": [],
   "source": [
    "batch_size = 10 # batch size\n",
    "number_of_batches = ceil(len(data['sentence']) / batch_size)\n",
    "DUMP_SIZE = 100 # number of batches to be dumped\n",
    "batched_sentences = np.array_split(data['sentence'].values, number_of_batches)\n",
    "number_of_files = ceil(number_of_batches / DUMP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mYmEU9SL0UwI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a4ad2885c54210a89dd77ee6f94fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating topological features:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats_name = \"s_e_v_c_b0b1_m_k\" # \"c\"\n",
    "stats_cap = 500\n",
    "stats_tuple_lists_array = []\n",
    "for i, filename in enumerate(tqdm(adj_filenames, \n",
    "                                  desc='Calculating topological features')):\n",
    "    with gzip.GzipFile(filename, 'rb') as f:\n",
    "        adj_matricies = np.load(f, allow_pickle=True)\n",
    "        ntokens = ntokens_array[i*batch_size*DUMP_SIZE : (i+1)*batch_size*DUMP_SIZE]\n",
    "    if debug:\n",
    "        stats_tuple_lists_array_part = count_top_stats(adj_matricies, thresholds_array, ntokens, stats_name.split(\"_\"), stats_cap, verbose=2) \n",
    "    else:\n",
    "        splitted = split_matricies_and_lengths(adj_matricies, ntokens, num_of_workers)\n",
    "        args = [(m, thresholds_array, ntokens, stats_name.split(\"_\"), stats_cap) for m, ntokens in splitted]\n",
    "        stats_tuple_lists_array_part = pool.starmap(\n",
    "            count_top_stats, args\n",
    "        )\n",
    "    stats_tuple_lists_array.append(np.concatenate([_ for _ in stats_tuple_lists_array_part], axis=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CG3s3Vaw0d5w"
   },
   "outputs": [],
   "source": [
    "stats_tuple_lists_array = np.concatenate(stats_tuple_lists_array, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Yvcgp6TU0sdX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘features’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fZQDK89-8jaD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/features/test_subs_e_v_c_b0b1_m_k_array_6.npy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8kMbYKkB0tw_"
   },
   "outputs": [],
   "source": [
    "np.save(stats_file, np.concatenate(stats_tuple_lists_array, axis=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-99873f36c864e5e9\n",
      "Found cached dataset csv (/home/jupyter-test/.cache/huggingface/datasets/csv/default-99873f36c864e5e9/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 596.88it/s]\n",
      "Template Feature Calc: 100%|██████████████████████| 1/1 [00:16<00:00, 16.82s/it]\n"
     ]
    }
   ],
   "source": [
    "model_dir_ = \"./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/\"\n",
    "for data_subset in [\"test_sub\"]:\n",
    "    d_dir = f\"./data/en-cola/{data_subset}.csv\"\n",
    "    !python src.template_features.py --model_dir $model_dir_ --data_file $d_dir --num_of_workers 40"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
