{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notebook for fine-tuning encoder-based LMs on LA task.  \n",
        "**Running this code requires GPU**  \n",
        "T4 availiable in Free Colab is sufficient to fine-tune encoder LMs on datasets in (```./data/```).  \n",
        "Average fine-tuning time per epoch: ~2 minutes.  \n",
        "Storing the weights of the model, all predictions and uncompressed attentions requires around 1Gb of free memory."
      ],
      "metadata": {
        "id": "Vm_dl4vHfOpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## When running in colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# %cd /content/gdrive/My Drive"
      ],
      "metadata": {
        "id": "rGAD0Nw-cT1p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/upunaprosk/la-tda.git"
      ],
      "metadata": {
        "id": "70Q3nsxobOwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhGAT10AWkdO"
      },
      "outputs": [],
      "source": [
        "%cd la-tda\n",
        "!unzip data/data.zip -d ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OoIn_zHInuCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee6a5b1-0379-489f-90db-0a420510f238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ripserplusplus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7VXEbJc2eybX"
      },
      "outputs": [],
      "source": [
        "from src.grab_attentions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHgVLinqU9WL"
      },
      "source": [
        "# Fine-tuning a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSY-IKWIVP9g",
        "outputId": "3c869c25-a475-4223-ab86-4bb2e83cbf65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 13 10:15:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KyiOQFaPN7Ew"
      },
      "outputs": [],
      "source": [
        "epoch = 1\n",
        "lr = 3e-5\n",
        "decay = 1e-2\n",
        "batch=32\n",
        "model_save_dir = \"./\"\n",
        "run_name = f\"bert-base-cased-en-cola_{batch}_{lr}_lr_{decay}_decay_balanced\"\n",
        "output_dir = model_save_dir+run_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL8_AwTgWi9_"
      },
      "source": [
        "## Training argumens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjJP0MofWxgR"
      },
      "source": [
        "Training argumens include  \n",
        "\n",
        "\n",
        "* [Trainer](https://github.com/huggingface/transformers/blob/d92e22d1f28324f513f3080e5c47c071a3916721/src/transformers/training_args.py#L121) class parameters;  \n",
        "* Model type arguments;  \n",
        "```\n",
        "  --model_name_or_path MODEL_NAME_OR_PATH\n",
        "                        Path to pretrained model or model identifier from\n",
        "                        huggingface.co/models (default: None)\n",
        "  --config_name CONFIG_NAME\n",
        "                        Pretrained config name or path if not the same as\n",
        "                        model_name (default: None)\n",
        "  --tokenizer_name TOKENIZER_NAME\n",
        "                        Pretrained tokenizer name or path if not the same as\n",
        "                        model_name (default: None)\n",
        "```\n",
        "* Data training arguments;\n",
        "```\n",
        "  --task_name TASK_NAME\n",
        "                        The name of the task to train on: cola, mnli, mrpc,\n",
        "                        qnli, qqp, rte, sst2, stsb, wnli (default: None)\n",
        "  --dataset_name DATASET_NAME\n",
        "                        The name of the dataset to use (via the datasets\n",
        "                        library). (default: None)\n",
        "  --max_seq_length MAX_SEQ_LENGTH\n",
        "                        The maximum total input sequence length after\n",
        "                        tokenization. Sequences longer than this will be\n",
        "                        truncated, sequences shorter will be padded. (default:\n",
        "                        128)\n",
        "  --train_file TRAIN_FILE\n",
        "                        A csv or a json file containing the training data.\n",
        "                        (default: None)\n",
        "  --validation_file VALIDATION_FILE\n",
        "                        A csv or a json file containing the validation data.\n",
        "                        (default: None)\n",
        "  --test_file TEST_FILE\n",
        "                        A csv or a json file containing the test data.\n",
        "                        (default: None)\n",
        "  --output_dir OUTPUT_DIR\n",
        "                        The output directory where the model predictions and\n",
        "                        checkpoints will be written. (default: None)\n",
        "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
        "                        Overwrite the content of the output directory. Use\n",
        "                        this to continue training if output_dir points to a\n",
        "                        checkpoint directory. (default: False)\n",
        "  --do_train [DO_TRAIN]\n",
        "                        Whether to run training. (default: False)\n",
        "  --do_eval [DO_EVAL]   Whether to run eval on the dev set. (default: False)\n",
        "  --do_predict [DO_PREDICT]\n",
        "                        Whether to run predictions on the test set. (default:\n",
        "                        False)\n",
        "  --evaluation_strategy {no,steps,epoch}\n",
        "                        The evaluation strategy to use. (default: no)\n",
        "```\n",
        "\n",
        "\n",
        "* Balance loss function;\n",
        "```\n",
        "  --balance_loss        Whether to use class-balanced loss. (default: False)\n",
        "```\n",
        "* Layers weights freezing;  \n",
        "```\n",
        "  --freeze              Whether to use pre-trained model without fine-tuning.\n",
        "                        (default: False)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueec-1WBdfKY",
        "outputId": "71284477-0cde-49c5-a39a-e3a68027f4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-13 10:15:17.563073: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-13 10:15:18.552272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/la-tda/src/train.py:48: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  ACCURACY = load_metric(\"accuracy\", keep_in_memory=True)\n",
            "Downloading builder script: 4.21kB [00:00, 3.37MB/s]       \n",
            "Downloading builder script: 4.47kB [00:00, 3.46MB/s]       \n",
            "10/13/2023 10:15:21 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/13/2023 10:15:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/runs/Oct13_10-15-21_536f80077e44,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            "xpu_backend=None,\n",
            ")\n",
            "10/13/2023 10:15:21 - INFO - __main__ - load a local file for train: data/en-cola/train.csv\n",
            "10/13/2023 10:15:21 - INFO - __main__ - load a local file for validation: data/en-cola/dev.csv\n",
            "10/13/2023 10:15:21 - INFO - __main__ - load a local file for test: data/en-cola/test.csv\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=None' instead.\n",
            "  warnings.warn(\n",
            "Using custom data configuration default-e876381b9e617786\n",
            "10/13/2023 10:15:21 - INFO - datasets.builder - Using custom data configuration default-e876381b9e617786\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
            "10/13/2023 10:15:21 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
            "Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "10/13/2023 10:15:21 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n",
            "10/13/2023 10:15:21 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 14665.40it/s]\n",
            "Downloading took 0.0 min\n",
            "10/13/2023 10:15:21 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "10/13/2023 10:15:21 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1633.72it/s]\n",
            "Generating train split\n",
            "10/13/2023 10:15:21 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 8551 examples [00:00, 124686.66 examples/s]\n",
            "Generating validation split\n",
            "10/13/2023 10:15:22 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 527 examples [00:00, 88817.38 examples/s]\n",
            "Generating test split\n",
            "10/13/2023 10:15:22 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 516 examples [00:00, 86508.15 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "10/13/2023 10:15:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n",
            "10/13/2023 10:15:22 - INFO - datasets.builder - Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n",
            "(…)bert-base-cased/resolve/main/config.json: 100% 570/570 [00:00<00:00, 2.90MB/s]\n",
            "[INFO|configuration_utils.py:651] 2023-10-13 10:15:22,345 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
            "[INFO|configuration_utils.py:703] 2023-10-13 10:15:22,350 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.23.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "(…)cased/resolve/main/tokenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 190kB/s]\n",
            "[INFO|configuration_utils.py:651] 2023-10-13 10:15:22,560 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
            "[INFO|configuration_utils.py:703] 2023-10-13 10:15:22,560 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.23.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "(…)o/bert-base-cased/resolve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 4.91MB/s]\n",
            "(…)t-base-cased/resolve/main/tokenizer.json: 100% 436k/436k [00:00<00:00, 24.4MB/s]\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-10-13 10:15:22,895 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-10-13 10:15:22,895 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-10-13 10:15:22,896 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-10-13 10:15:22,896 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-10-13 10:15:22,896 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:651] 2023-10-13 10:15:22,896 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
            "[INFO|configuration_utils.py:703] 2023-10-13 10:15:22,897 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.23.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 436M/436M [00:02<00:00, 210MB/s]\n",
            "[INFO|modeling_utils.py:2088] 2023-10-13 10:15:25,205 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2528] 2023-10-13 10:15:26,788 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2540] 2023-10-13 10:15:26,788 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/8551 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-9e6a2f1eeebd5594.arrow\n",
            "10/13/2023 10:15:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-9e6a2f1eeebd5594.arrow\n",
            "Running tokenizer on dataset: 100% 8551/8551 [00:00<00:00, 15012.14 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/527 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-47737ae0b6deb9d4.arrow\n",
            "10/13/2023 10:15:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-47737ae0b6deb9d4.arrow\n",
            "Running tokenizer on dataset: 100% 527/527 [00:00<00:00, 13830.03 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/516 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-192c7a39696f8069.arrow\n",
            "10/13/2023 10:15:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e876381b9e617786/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-192c7a39696f8069.arrow\n",
            "Running tokenizer on dataset: 100% 516/516 [00:00<00:00, 11186.02 examples/s]\n",
            "10/13/2023 10:15:27 - INFO - __main__ - Sample 1824 of the training set: {'code': 'r-67', 'label': 0, 'judgement': '?*', 'sentence': 'I acknowledged that my father, he was tight as an owl.', 'input_ids': [101, 146, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "10/13/2023 10:15:27 - INFO - __main__ - Sample 409 of the training set: {'code': 'bc01', 'label': 1, 'judgement': None, 'sentence': 'For him to do that would be a mistake.', 'input_ids': [101, 1370, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "10/13/2023 10:15:27 - INFO - __main__ - Sample 4506 of the training set: {'code': 'ks08', 'label': 1, 'judgement': None, 'sentence': 'Mary sang a song, but Lee never did.', 'input_ids': [101, 2090, 6407, 170, 1461, 117, 1133, 2499, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "10/13/2023 10:15:27 - INFO - __main__ - Assigned weights to classes\n",
            "10/13/2023 10:15:27 - INFO - __main__ - {1.6912579113924051: 0, 0.7098621949194753: 1}\n",
            "[INFO|trainer.py:725] 2023-10-13 10:15:34,457 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2023-10-13 10:15:34,468 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2023-10-13 10:15:34,468 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1609] 2023-10-13 10:15:34,468 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1610] 2023-10-13 10:15:34,469 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1611] 2023-10-13 10:15:34,469 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1612] 2023-10-13 10:15:34,469 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2023-10-13 10:15:34,469 >>   Total optimization steps = 268\n",
            "100% 268/268 [01:31<00:00,  3.72it/s][INFO|trainer.py:1852] 2023-10-13 10:17:05,674 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 91.2118, 'train_samples_per_second': 93.749, 'train_steps_per_second': 2.938, 'train_loss': 0.5616749436108034, 'epoch': 1.0}\n",
            "100% 268/268 [01:31<00:00,  2.94it/s]\n",
            "[INFO|trainer.py:2656] 2023-10-13 10:17:05,682 >> Saving model checkpoint to ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced\n",
            "[INFO|configuration_utils.py:445] 2023-10-13 10:17:05,683 >> Configuration saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/config.json\n",
            "[INFO|modeling_utils.py:1583] 2023-10-13 10:17:06,848 >> Model weights saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-10-13 10:17:06,849 >> tokenizer config file saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-10-13 10:17:06,849 >> Special tokens file saved in ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     0.5617\n",
            "  train_runtime            = 0:01:31.21\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =     93.749\n",
            "  train_steps_per_second   =      2.938\n",
            "10/13/2023 10:17:06 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:725] 2023-10-13 10:17:06,885 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-10-13 10:17:06,887 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-10-13 10:17:06,887 >>   Num examples = 527\n",
            "[INFO|trainer.py:2912] 2023-10-13 10:17:06,887 >>   Batch size = 8\n",
            "100% 66/66 [00:01<00:00, 33.61it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.8368\n",
            "  eval_loss               =     0.4646\n",
            "  eval_mcc                =     0.6041\n",
            "  eval_runtime            = 0:00:01.99\n",
            "  eval_samples            =        527\n",
            "  eval_samples_per_second =    264.547\n",
            "  eval_steps_per_second   =     33.131\n",
            "10/13/2023 10:17:08 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:725] 2023-10-13 10:17:08,883 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-10-13 10:17:08,884 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-10-13 10:17:08,884 >>   Num examples = 8551\n",
            "[INFO|trainer.py:2912] 2023-10-13 10:17:08,884 >>   Batch size = 8\n",
            "100% 1069/1069 [00:32<00:00, 33.34it/s]\n",
            "10/13/2023 10:17:40 - INFO - __main__ - ***** Predict results train *****\n",
            "[INFO|trainer.py:725] 2023-10-13 10:17:40,981 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-10-13 10:17:40,984 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-10-13 10:17:40,984 >>   Num examples = 527\n",
            "[INFO|trainer.py:2912] 2023-10-13 10:17:40,984 >>   Batch size = 8\n",
            "100% 66/66 [00:01<00:00, 33.53it/s]\n",
            "10/13/2023 10:17:42 - INFO - __main__ - ***** Predict results dev *****\n",
            "[INFO|trainer.py:725] 2023-10-13 10:17:42,985 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: judgement, code, sentence. If judgement, code, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2023-10-13 10:17:42,987 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-10-13 10:17:42,987 >>   Num examples = 516\n",
            "[INFO|trainer.py:2912] 2023-10-13 10:17:42,988 >>   Batch size = 8\n",
            "100% 65/65 [00:01<00:00, 33.48it/s]\n",
            "10/13/2023 10:17:44 - INFO - __main__ - ***** Predict results test *****\n",
            "[INFO|modelcard.py:443] 2023-10-13 10:17:45,274 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8368121442125237}]}\n"
          ]
        }
      ],
      "source": [
        "!python src/train.py \\\n",
        "        --model_name_or_path bert-base-cased \\\n",
        "        --train_file data/en-cola/train.csv \\\n",
        "        --validation_file data/en-cola/dev.csv \\\n",
        "        --test_file data/en-cola/test.csv \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --do_predict\\\n",
        "        --num_train_epochs $epoch\\\n",
        "        --learning_rate $lr\\\n",
        "        --weight_decay $decay\\\n",
        "        --max_seq_length 64\\\n",
        "        --per_device_train_batch_size $batch\\\n",
        "        --output_dir $output_dir\\\n",
        "        --balance_loss\\\n",
        "        # --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeCSemL-ZLhR"
      },
      "source": [
        "# Attention weights extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a subset for which to extract attention matrices. Further will be used for feature calculation\n",
        "# Avaliable subsets:\n",
        "!find data/ -type f -exec ls -lhS {} \\;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK4NTSQUdXKm",
        "outputId": "d2b95725-9e74-4437-b026-d986eee27105"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 184K Sep  7  2022 data/en-cola/phenomena_minor.tsv\n",
            "-rw-r--r-- 1 root root 26K Sep  7  2022 data/en-cola/dev.csv\n",
            "-rw-r--r-- 1 root root 421K Sep  7  2022 data/en-cola/train.csv\n",
            "-rw-r--r-- 1 root root 28K Sep  7  2022 data/en-cola/test.csv\n",
            "-rw-r--r-- 1 root root 86K Oct  9  2022 data/en-cola/phenomena.tsv\n",
            "-rw-r--r-- 1 root root 118K Sep 11  2022 data/ru-cola/dev.csv\n",
            "-rw-r--r-- 1 root root 964K Sep 11  2022 data/ru-cola/train.csv\n",
            "-rw-r--r-- 1 root root 296K Sep 11  2022 data/ru-cola/test.csv\n",
            "-rw-r--r-- 1 root root 420K Oct  9  2022 data/ru-cola/phenomena.csv\n",
            "-rw-r--r-- 1 root root 929K Oct 13 10:13 data/data.zip\n",
            "-rw-r--r-- 1 root root 58K Sep  7  2022 data/ita-cola/dev.csv\n",
            "-rw-r--r-- 1 root root 491K Sep  7  2022 data/ita-cola/train.csv\n",
            "-rw-r--r-- 1 root root 62K Sep  7  2022 data/ita-cola/test.csv\n",
            "-rw-r--r-- 1 root root 159K Oct  9  2022 data/ita-cola/phenomena.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xJ1Tn9hFS-JT"
      },
      "outputs": [],
      "source": [
        "d_dir = \"./data/en-cola/dev.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HBNhb8lETCZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28ee32c-25c3-43c8-af72-a580d1d714e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "Moving 0 files to the new cache system\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "\u001b[32m[I] Loading csv dataset from path: data/en-cola/dev.csv...\u001b[0m\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 8756.38it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 977.24it/s]\n",
            "Generating dev split: 527 examples [00:00, 54978.19 examples/s]\n",
            "\u001b[32m[I] CUDA is available : True\u001b[0m\n",
            "\u001b[33m[W] Using cuda\u001b[0m\n",
            "\u001b[32m[I] CUDA version : 11.8\u001b[0m\n",
            "\u001b[32m[I] PyTorch version : 2.0.1+cu118\u001b[0m\n",
            "\u001b[32m[I] Loading model from ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced...\u001b[0m\n",
            "\u001b[32m[I] Loading tokenizer from ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced...\u001b[0m\n",
            "Weights Extraction: 100% 53/53 [00:05<00:00, 10.05it/s]\n",
            "\u001b[32m[I] Saving weights to: bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/attentions/dev_part1of1.npy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH=%PYTHONPATH% python -m src.grab_attentions --model_dir $output_dir --data_file $d_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rowE-Mk0TMUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2012f50b-dfac-4478-b8b2-b801bcb1c97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[I] Loading csv dataset from path: data/en-cola/dev.csv...\u001b[0m\n",
            "\u001b[32m[I] CUDA is available : True\u001b[0m\n",
            "\u001b[33m[W] Using cuda:0\u001b[0m\n",
            "\u001b[32m[I] CUDA version : 11.8\u001b[0m\n",
            "\u001b[32m[I] PyTorch version : 2.0.1+cu118\u001b[0m\n",
            "\u001b[32m[I] Loading model from ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced...\u001b[0m\n",
            "\u001b[32m[I] Loading tokenizer from ./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Weights Extraction: 100%|██████████| 53/53 [00:04<00:00, 12.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[I] Saving weights to: bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/attentions/dev_part1of1.npy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Ex. of direct function call\n",
        "grab_attention_weights_inference(output_dir, d_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -c -h $output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNJ-u9sxiyIM",
        "outputId": "a8b192a8-8537-40d1-a239-df646b54de38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12K\t./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/runs/Oct13_10-15-21_536f80077e44/1697192134.4824107\n",
            "28K\t./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/runs/Oct13_10-15-21_536f80077e44\n",
            "32K\t./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/runs\n",
            "593M\t./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced/attentions\n",
            "1008M\t./bert-base-cased-en-cola_32_3e-05_lr_0.01_decay_balanced\n",
            "1008M\ttotal\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}